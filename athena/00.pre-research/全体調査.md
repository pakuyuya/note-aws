# 概要

AWS Athenaに関する事前調査まとめ

## Amazon Athenaとは

S3に配置したデータを直接クエリするAWSのサービス。
巷ではフルマネージドHIVEとかいわれている。

* S3 に置いたファイルを直接検索。1TB読み取りにより$5。ファイルが圧縮されていると更に低減

## チュートリアル

※要:ELB利用とアクセスログ

https://docs.aws.amazon.com/ja_jp/athena/latest/ug/getting-started.html

* SQLライクなクエリを流す
* TABLEに、S3ファイル名のprefixやpathをもたせるみたいだ。

## CREATE TABLEのリファレンスから、機能を読み解く

https://docs.aws.amazon.com/ja_jp/athena/latest/ug/create-table.html

```sql
CREATE [EXTERNAL] TABLE [IF NOT EXISTS]
 [db_name.]table_name [(col_name data_type [COMMENT col_comment] [, ...] )]
 [COMMENT table_comment]
 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
 [ROW FORMAT row_format]
 [STORED AS file_format] 
 [WITH SERDEPROPERTIES (...)] ]
 [LOCATION 's3_loc']
 [TBLPROPERTIES ( ['has_encrypted_data'='true | false',] ['classification'='aws_glue_classification',] property_name=property_value [, ...] ) ]
 ```

 ### [PARTITIONED BY (col_name data_type [ COMMENT col_comment ], ... ) ]

* Athenaにはテーブルごとにパーティションという概念があり、
  要するにデータの範囲付きでS3のファイルLOCATIONを追加できる。
* 下記サイトがよくわかる。
  https://dev.classmethod.jp/cloud/aws/athena-partition-reinvent/

* SQLのWHERE句に条件指定することでスキャン範囲が限定できるらしい。
* S3のサブフォルダを（列名=値）という形式で命名しておき、`PERTITIONED BY` に指定すれば自動追加できる。
  なお、PERTIETIONED BY に指定した列はテーブルの列に加わる。

* 次のサンプルが詳しい。
  https://docs.aws.amazon.com/ja_jp/athena/latest/ug/partitions.html

###  [ROW FORMAT row_format]

* データ形式を指定できる。
* 指定の仕方は非常に柔軟かつ覚えにくい。公式ドキュメントに頼る。
  https://docs.aws.amazon.com/ja_jp/athena/latest/ug/supported-format.html
* 中には、テキストを柔軟にパースするという恐ろしいものもある。

* また一部のフォーマットはcolumnerなフォーマットで、クエリに指定した列だけをスキャンしトータルのスキャンサイズを削減できる効果がある。（下記、検証記事）
  https://dev.classmethod.jp/cloud/aws/amazon-athena-using-parquet/
* そして、Parquetは、データに列名を持つことができる。つまり簡易スキーマになる。
* Parquetの変換が面倒そうに思えたが、AWS Glue（S3toS3のETLサービス)がおあつらえだそうだ。
  https://qiita.com/hideji2/items/85747e3d66026045614d

## つかってみよう

### S3にCSVを配置して、Athenaのコンソールからクエリを投げる

```sql
CREATE EXTERNAL TABLE test (
    num int,
    some string,
    one string
    )
ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
      ESCAPED BY '"'
      LINES TERMINATED BY '\n'
LOCATION 's3://paku-test-bucket/athena/test/';

-- Put file to s3://paku-test-bucket/athena/test/test.csv
```

### S3にParquetファイルを置いてS3に配置

事前にS3にParquet形式のファイルを配置する


* Pythonを用いた CSV->Parquetファイル変換 https://qiita.com/shiumachi/items/59c332f09a8c4c440f93

```sql
CREATE EXTERNAL TABLE test_parquet (
    num int,
    some string,
    one string
    )
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
WITH SERDEPROPERTIES (
  'serialization.format' = '1'
) LOCATION 's3://paku-test-bucket/athena/test_parquet/';
select * from test_parquet;

-- Put file to s3://paku-test-bucket/athena/test_parquet/test.parquet
```
